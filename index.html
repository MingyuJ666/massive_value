<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Deformable Neural Radiance Fields creates free-viewpoint portraits (nerfies) from casually captured videos.">
  <meta name="keywords" content="Nerfies, D-NeRF, NeRF">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Massive Values in Self-Attention Modules are the Key to Contextual Knowledge Understanding</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/disk.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://mingyuj666.github.io/">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://github.com/MingyuJ666/The-Impact-of-Reasoning-Step-Length-on-Large-Language-Models">
            The Impact of Reasoning Step Length on Large Language Models
          </a>
          <a class="navbar-item" href="https://tiuxuxsh76075.github.io/prollm.github.io/">
            ProLLM: Protein Chain-of-Thoughts Enhanced LLM for Protein-Protein Interaction Prediction
          </a>
          <a class="navbar-item" href="https://luckfort.github.io/explore_CD/">
            Exploring Concept Depth: How Large Language Models Acquire Knowledge at Different Layers?
          </a>
        </div>
      </div>
    </div>

  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Massive Values in Self-Attention Modules are the Key to Contextual Knowledge Understanding</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://mingyuj666.github.io/">Mingyu Jin</a><sup>1</sup>,</span>
            <span class="author-block">
               Kai Mei</a><sup>1</sup>,</span>
              <a href="https://wujiangxu.github.io/">Wujiang Xu</a><sup>1</sup>,</span>
              <a href="https://eric-mingjie.github.io/">Mingjie Sun</a><sup>2</sup>,</span>
              <a href="https://www.ruixiangtang.net/home">Ruixiang Tang</a><sup>1</sup>,</span>
              <a href="https://mengnandu.com/">Mengnan Du</a><sup>4</sup>,</span>
            <span class="author-block">
              <a href="https://zirui-ray-liu.github.io/">Zirui Liu</a><sup>3</sup></span>
              <a href="https://yongfeng.me/">Yongfeng Zhang</a><sup>1</sup></span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Rutgers University,</span>
            <span class="author-block"><sup>2</sup>Carnegie Mellon University,</span>
            <span class="author-block"><sup>3</sup>University of Minnesota,</span>
            <span class="author-block"><sup>4</sup>New Jersey Institute of Technology,</span>
          </div>
          
         

          <div class="is-size-5 publication-authors">
            <span class="author-block"><b>Accepted by ICML 2025</b></span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2502.01563"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <img src="./static/images/icml.png" alt="Accepted by ICML" style="width: 40px; height: 16px;"> <!-- ‰øÆÊîπÂõæÁâáÈ´òÂ∫¶ -->
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/MingyuJ666/Rope_with_LLM"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="main image">
  <style type="text/css">
    body{
      background: no-repeat center center fixed;
                  -webkit-background-size: cover;
                  -o-background-size: cover;                
                  background-size: cover;
    }
  </style>

  <div class="image-body">
    <div align=center>
      <img src="./static/images/Figure1.png" alt="Introduction" style="width: 37%;">
      <div class="columns is-centered has-text-centered">
        <div class="column is-three-fifths">
          <h2 class="subtitle has-text-centered">
            In transformer-based Large Language Models with RoPE(like Llama, Gemma), the attention queries (Q) and keys (K) exhibit concentrated massive values in certain dimensions.
          </h2>
        </div>
      </div>
    </div>
  </div>

  <br /> <br />

  <div class="image-body">
    <div align=center>
      <img src="./static/images/Figure2.png" alt="Introduction" style="width: 55%;">
      <div class="columns is-centered has-text-centered">
        <div class="column is-three-fifths">
          <h2 class="subtitle has-text-centered">
            Q and K Embedding Vector in Llama-2-7B, we choose Layer 10 and 20. This visualization shown here is a two-dimensional image because we averaged over the sequence-length dimension. The horizontal axis is the number of heads, and the vertical axis is head dim. We can see that the massive value is concentrated at the bottom of the picture.
          </h2>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          Large language models (LLMs) have achieved remarkable success in contextual knowledge understanding. In this paper, we show for the first time that these concentrated massive values consistently emerge in specific regions of attention queries (Q) and keys (K) while not having such patterns in values (V) in various modern transformer-based LLMs. Through extensive experiments, we further demonstrate that these massive values play a critical role in interpreting contextual knowledge (i.e., knowledge obtained from the current context window) rather than in retrieving parametric knowledge stored within the model‚Äôs parameters. Our further investigation of quantization strategies reveals that ignoring these massive values leads to a pronounced drop in performance on tasks requiring rich contextual understanding, aligning with our analysis. Finally, we trace the emergence of concentrated massive values and find that such concentration is caused by Rotary Positional Encoding (RoPE) and it appears since very first layers. These findings shed new light on how Q and K operate in LLMs and offer practical insights for model design and optimization. 
            </a>
          </p>
          
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>


  <br /> <br />


  <!--Main Body-->

  <div class="columns is-centered has-text-centered">
  <div class="column is-four-fifths">
    <h2 class="title is-3">üîç Key Findings</h2>

    <div class="content has-text-justified">
      <p align="center">We highlight three core findings from our study of massive values in LLM attention mechanisms.</p>

      <h4 class="title is-5" align="center">üìä Functional Role of Massive Values</h4>
      <p align="center">
        We conduct extensive experiments that show that massive values in <code>Q</code> and <code>K</code> matrices play a crucial role in contextual knowledge retrieval, while having a limited effect on parametric knowledge.
      </p>

      <h4 class="title is-5" align="center">‚öôÔ∏è Impact on Quantization</h4>
      <p align="center">
        We evaluate 3 quantization strategies and find that methods explicitly addressing massive values better preserve contextual understanding. This suggests the need for quantization-aware designs.
      </p>

      <h4 class="title is-5" align="center">‚è±Ô∏è Temporal Origin Analysis</h4>
      <p align="center">
        Through causal and temporal analysis, we show that massive values originate from the RoPE mechanism and appear as early as the initial layers. 
      </p>
    </div>
  </div>
</div>



  <!-- Case Example -->
    
  <div class="columns is-centered has-text-centered">
    <div class="column is-four-fifths">
      <h2 class="title is-3">üëÄCase Examples</h2>
      
      <div class="content has-text-justified">
        <h4 class="title is-4" align="center">Fact: Cities</h4>

        <p>
          ‚úÖ Judge the statement is True or False.
          <font color="#0000FF">
            The city of Tokyo is in Japan.
          </font>
        </p>
        <p>
          ‚ùå Judge the statement is True or False.
          <font color="#0000FF">
            The city of Lodz is in the Dominican Republic.
          </font> 
        </p>

        <h4 class="title is-4" align="center">Emotional: HateEval</h4>

        <p>
          ‚úÖ
          <font color="#0000FF">
            Here it is not about Refugees or Illegal immigrants. It is about whether one has documents before 1971 or not. Now, it is difficult for slum people and beggars to show valid documents, except the name in voter list. 
          </font>
          According to the comment, tell whether they present hate speech or not.
        </p>
        <p>
          ‚ùå
          <font color="#0000FF">
           Labor migrants transfer almost $10 billion a year to Ukraine.
          </font> According to the comment, tell whether they present hate speech or not.
        </p>

        <h4 class="title is-4" align="center">Reasoning: Coin-Flip</h4>

        <p>
          ‚úÖ
          <font color="#0000FF">
            A coin is heads up. Whitney flips the coin. Erika does not flip the coin. Tj does not flip the coin. Benito flips the coin. Is the coin still heads up? Note that "flip" here means "reverse".
          </font>
          According to the flipping process above, determine if a coin remains heads up after it is either flipped or left unflipped by individuals. Therefore, the answer (Yes or No) is?
        </p>
        <p>
          ‚ùå
          <font color="#0000FF">
            A coin is heads up. Lucky does not flip the coin. Mireya flips the coin. Jj flips the coin. Kc flips the coin. Is the coin still heads up? Note that "flip" here means "reverse".
          </font>
          According to the flipping process above, determine if a coin remains heads up after it is either flipped or left unflipped by individuals. Therefore, the answer (Yes or No) is?
        </p>

        
      </div>
    </div>
  </div>
  <!--/ Case Example -->

  <!-- Anchor. -->
    
  <div class="columns is-centered has-text-centered">
    <div class="column is-four-fifths">
      <h2 class="title is-3">‚öìÔ∏èAnchoring Difficulties</h2>

      <div class="image-body">
        <div align=center>
          <img src="./static/images/Anchor_avg.png" alt="CD Introduction" style="width: 30%;">
          <div class="columns is-centered has-text-centered">
            <div class="column is-three-fifths">
              <h2 class="subtitle has-text-centered">
                The dataset with the highest accuracy, <span class="dnerf">IMDb</span>, is deemed the easiest dataset to classify. Conversely, the dataset with the lowest accuracy, <span class="dnerf">Coin-Flip</span>, is considered the most difficult to classify.
              </h2>
            </div>
          </div>
          
        </div>
      </div>
      
      <br /> <br />
      <div class="content has-text-justified">
        <p>
          To ascertain the learning difficulty of each dataset, we have utilized the 
          <a href="https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct">
            <span>LlaMA3-8B-Instruct</span>
          </a>,
          <a href="https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/">
            <span>GPT-4o-mini</span>
          </a>, and
          <a href="https://huggingface.co/Qwen/Qwen2-7B-Instruct">
            <span>QWen2-7B-Instruct</span>
          </a>
          model. 
          Our approach involves testing each sample in the datasets as a binary classification problem via a prompting way. 
        </p>
        <p>
          The model generates a response for each sample, from which we infer a judgment, categorizing it as either "Yes" or "No". 
          By comparing these judgments with the actual labels, we compute the accuracy for each dataset.
        </p>
        
      </div>
    </div>
  </div>
  <!--/ Anchor. -->


  <!-- Experiment. -->
    
  <div class="columns is-centered has-text-centered">
    <div class="column is-four-fifths">
      <h2 class="title is-3">üìäExperiments</h2>
      
      <div class="content has-text-justified">
        <p><b>RQ1: Do different LLMs' Concept Depths behave consistently in the same dataset?</b></p>
    
        <p>We categorize the performances into three types.
          1) For Cities, STSA, IMDb, and Sarcasm, the LLMs suddenly understand the tasks at intermediate layers. 
          2) For CommonClaim and HateEval, the LLMs have already understood the tasks in shallower layers. 
          3) For Counterfact, StrategyQA, and Coinflip, The tasks are more difficult to understand compared with others. Therefore, we consider the tasks in type 1 and 2 easy tasks, and those in type 3 are complex. 
        </p>
        
      </div>

        <div class="image-body">
          <div align=center>
            <img src="./static/images/all.png" alt="CD Introduction" style="width: 70%;">
            <div class="columns is-centered has-text-centered">
              <div class="column is-three-fifths">
                <h2 class="subtitle has-text-centered">
                  Linear probing accuracy of Gemma-7B, LLaMA-7B, Qwen-7B on nine datasets.
                </h2>
              </div>
            </div>
          </div>
        </div>

  <br /> <br />
  
  <div class="content has-text-justified">
        
    <p><b>RQ2: Do different size LLMs in the same family <i>(e.g., the LLaMA family)</i> have consistent Concept Depth?</b></p>

    <p>
      We have two observations by comparing different sizes of models from the same LLM family. 1) As the number of parameters increases, peak accuracy gradually increases, and the converging point gradually advances. 2) Larger models grasp the concepts earlier and better.
    </p>
    
  </div>
      
  <div class="image-body">
    <div align=center>
      <img src="./static/images/peak.png" alt="CD Introduction" style="width: 100%;">
      <div class="columns is-centered has-text-centered">
        <div class="column is-three-fifths">
          <h2 class="subtitle has-text-centered">
            The peak accuracy of each dataset on Gemma, LLaMA, and Qwen represented by the percent depth proportion.
          </h2>
        </div>
      </div>
    </div>
  </div>

  <br /> <br />
      
  <div class="image-body">
    <div align=center>
      <img src="./static/images/converge.png" alt="CD Introduction" style="width: 100%;">
      <div class="columns is-centered has-text-centered">
        <div class="column is-three-fifths">
          <h2 class="subtitle has-text-centered">
            The converging point of each dataset on Gemma, LLaMA, and Qwen represented by the percent depth proportion.
          </h2>
        </div>
      </div>
    </div>
  </div>

  <br /> <br />
      
      <div class="content has-text-justified">

        <p><b>RQ3: Do LLMs' Concept Depth of the same size behave consistently?</b></p>
        <p>
          With the same number of model parameters, the models generally have a comparable understanding of the datasets.
        </p>
        
        
      </div>
    
      <div class="image-body">
        <div align=center>
          <img src="./static/images/rq3.jpg" alt="CD Introduction" style="width: 70%;">
          <div class="columns is-centered has-text-centered">
            <div class="column is-three-fifths">
              <h2 class="subtitle has-text-centered">
                Linear probing accuracy of Gemma-7B, LLaMA-7B, Qwen-7B on nine datasets.
              </h2>
            </div>
          </div>
        </div>
      </div>
      
      <br/><br/>

      <div class="content has-text-justified">

        <p><b>Ablation Study: How can quantization (lower model precision) and noises (examing the robustness) affect LLM's Concept Depths?</b></p>
        <p>
          Noises or 8-bit-quantization can cause the accuracy to converge more slowly. Compressing the LLMs to 16 bits doesn't harm the understanding process too much.
The layer-wise representations of LLMs are susceptible to noise and high-ratio quantization. Therefore, it is crucial to proceed cautiously when conducting high-ratio quantization inference.
        </p>
        
      </div>

    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Conclusion. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Conclusion</h2>
        <div class="content has-text-justified">
          Our study provides novel insights into the role and origin of massive values in Large Language Models (LLMs). Through systematic investigation, we find that massive values are critical in contextual knowledge understanding tasks, such as passkey retrieval and IMDB sentiment understanding. In contrast, their influence on parametric knowledge retrieval tasks, such as world knowledge retrieval, is limited. This finding emphasizes the importance of preserving massive value to maintain model performance in reasoning and context-dependent tasks. Our investigation reveals that RoPE induces massive value stripes, distinct patterns exclusively in the Q and K, while absent in models without RoPE, such as OPT. This highlights how positional encoding mechanisms contribute to massive values, particularly low-frequency channel dimensions, offering new insights into RoPE's role in LLMs. This study establishes a deeper understanding of massive values in LLMs, their critical role in contextual knowledge understanding, their implications for model optimization techniques such as quantization, and their connection to RoPE-induced patterns. These findings lay the foundation for developing more robust, efficient, and interpretable LLM architectures and optimization strategies.
          
        </div>
      </div>
    </div>
    <!--/ Conclusion. -->
  </div>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{jin2025massive,
  title={Massive Values in Self-Attention Modules are the Key to Contextual Knowledge Understanding},
  author={Jin, Mingyu and Mei, Kai and Xu, Wujiang and Sun, Mingjie and Tang, Ruixiang and Du, Mengnan and Liu, Zirui and Zhang, Yongfeng},
  journal={arXiv preprint arXiv:2502.01563},
  year={2025}
}</code></pre>
  </div>
</section>




</body>
</html>
